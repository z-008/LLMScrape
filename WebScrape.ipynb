{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be1ba4d-1354-4abd-9b15-1dfa64e3795d",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": [
     "keep_outputs"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install requests\n",
    "!pip install html5lib\n",
    "!pip install bs4\n",
    "# !pip install langchain_community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa47d00-8042-40a7-afd2-51cfdcddec45",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10446164-0243-4eed-bb4d-14a8b2df0c3e",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install unstructured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fcfde96-6dfd-42b4-a867-c171dcb375f0",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f285679f-30cf-4fdc-91c7-7a67a84eba0a",
   "metadata": {},
   "source": [
    "## Libraries import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df96d35e-0390-4605-9d6c-e3b9db5c763e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import sys\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from langchain.document_loaders import ReadTheDocsLoader\n",
    "from langchain_community.document_loaders import BSHTMLLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14c139a-1778-4321-bf0f-bedcad3be6be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.agents.mrkl.base import ZeroShotAgent\n",
    "from langchain.agents import Tool\n",
    "from langchain.agents.agent import AgentExecutor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4e331a-048a-4568-8380-e2a4a68f28d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"../..\")\n",
    "from src.aws_utils import get_secrets\n",
    "from langchain.llms import OpenAI\n",
    "from src.prompts.prompt import REACT_PROMPT\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "secrets = get_secrets()\n",
    "# OPENAI_MODEL = \"gpt-3.5-turbo-instruct\"\n",
    "OpenAI.api_key = secrets[\"OPENAI_API_KEY\"]\n",
    "os.environ[\"OPENAI_API_KEY\"] = secrets[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1517a1b0-72e8-4bc4-82da-6b56f02d7c66",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "base_url = \"https://procogia.com/\"\n",
    "\n",
    "response = requests.get(base_url)\n",
    "soup = BeautifulSoup(response.content, \"html5lib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be9d782-0d8e-433a-bd61-7ae4fe5d3898",
   "metadata": {},
   "source": [
    "## Crawl all pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1928cef-2532-433e-8546-4303d1272efe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_links(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    links = soup.find_all('a', href=True)\n",
    "    return [link['href'] for link in links]\n",
    "\n",
    "def crawl_website(base_url, max_depth=3):\n",
    "    visited_urls = set()\n",
    "\n",
    "    def recursive_crawl(url, depth):\n",
    "        if depth > max_depth:\n",
    "            return\n",
    "\n",
    "        if url in visited_urls:\n",
    "            return\n",
    "\n",
    "        print(f\"Crawling: {url}\")\n",
    "        visited_urls.add(url)\n",
    "\n",
    "        links = get_links(url)\n",
    "        for link in links:\n",
    "            absolute_url = urljoin(base_url, link)\n",
    "            if urlparse(absolute_url).scheme in ['http', 'https']:\n",
    "                recursive_crawl(absolute_url, depth + 1)\n",
    "\n",
    "    recursive_crawl(base_url, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03bc8a9e-d60c-4c0d-a07e-ffa9f1ba4ff6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "crawl_website('https://procogia.com/', max_depth=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ca857b-f242-4e2e-993b-f6346d08deaa",
   "metadata": {},
   "source": [
    "## Crawl only ProCogia Pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85217dd3-dd0c-47e0-aafb-d88960fe75ba",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse\n",
    "\n",
    "def get_links(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    links = soup.find_all('a', href=True)\n",
    "    return [link['href'] for link in links]\n",
    "\n",
    "def crawl_website(base_url, max_depth=3, output_folder='crawled_pages'):\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    visited_urls = set()\n",
    "\n",
    "    def is_same_domain(url):\n",
    "        return urlparse(url).netloc == urlparse(base_url).netloc\n",
    "\n",
    "    def save_page(url, content):\n",
    "        filename = os.path.join(output_folder, f\"{urlparse(url).path.replace('/', '_')}.html\")\n",
    "        with open(filename, 'w', encoding='utf-8') as file:\n",
    "            file.write(content)\n",
    "\n",
    "    def recursive_crawl(url, depth):\n",
    "        if depth > max_depth or not is_same_domain(url):\n",
    "            return\n",
    "\n",
    "        if url in visited_urls:\n",
    "            return\n",
    "\n",
    "        print(f\"Crawling: {url}\")\n",
    "        visited_urls.add(url)\n",
    "\n",
    "        response = requests.get(url)\n",
    "        content = response.text\n",
    "\n",
    "        save_page(url, content)\n",
    "\n",
    "        links = get_links(url)\n",
    "        for link in links:\n",
    "            absolute_url = urljoin(base_url, link)\n",
    "            if is_same_domain(absolute_url):\n",
    "                recursive_crawl(absolute_url, depth + 1)\n",
    "\n",
    "    recursive_crawl(base_url, 0)\n",
    "    \n",
    "crawl_website('https://procogia.com', max_depth=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c08e4f9-f8f3-44f4-b53a-90868b449c4c",
   "metadata": {},
   "source": [
    "## Retrieval and Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27543796-0b83-4244-9fd0-f5aba067ef4b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "loader = DirectoryLoader(\"crawled_pages/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f893abc0-45a1-4dc4-9d8b-430f2eb02b89",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "raw_documents = loader.load()\n",
    "print(f\"loaded {len(raw_documents) } documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec03bad-4669-4306-9350-e8fe96bbd815",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, chunk_overlap=100, separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "        )\n",
    "documents = text_splitter.split_documents(documents=raw_documents)\n",
    "print(f\"Splitted into {len(documents)} chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20aa9f88-a4f2-4cc5-a7f0-61bfd5c30447",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "EMBEDDING_MODEL_PATH = \"intfloat/e5-base-v2\"\n",
    "\n",
    "# Get embedding model\n",
    "embeddings = SentenceTransformerEmbeddings(model_name=EMBEDDING_MODEL_PATH)\n",
    "\n",
    "# Use FAISS to create a database from the embeddings, with the original text\n",
    "docsearch = FAISS.from_documents(documents, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65cc352-f2e6-4d39-9aad-0981a1317990",
   "metadata": {},
   "source": [
    "## LLM- Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3437f596-d573-4871-8b27-f497ba5ef369",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "template = \"\"\"You are Chatbot, an assistant that offers support for answering questions related to ProCogia. \n",
    "Chatbot should assume that any query from the User will be regarding ProCogia.com, and so this information does not need to be added.\n",
    "If Chatbot thinks that the Question is completely irrelevant, then state that you are unable to answer that User query in a reasonable and friendly manner. \n",
    "You are a support bot for ProCogia first and foremost. \n",
    "Input: {question}\n",
    "Output: \n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.7, model_name=\"gpt-4\")\n",
    "memory = ConversationBufferMemory(\n",
    "memory_key='chat_history', return_messages=True)\n",
    "\n",
    "\n",
    "conversation_chain = ConversationalRetrievalChain.from_llm(\n",
    "        llm=llm,\n",
    "        retriever=docsearch.as_retriever(),\n",
    "        verbose = True,\n",
    "        memory=memory\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4650d96e-6970-4432-80bb-f92e91c1e9ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "retriever=docsearch.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74edcdf7-117f-4516-a957-9a6952016d26",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e5b484-2b4a-4ff5-913c-95ee83476f36",
   "metadata": {},
   "source": [
    "## Chain with GuardRail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf396fbc-36a7-4713-b15e-a63840aeb08b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def qa_chain_latest(question):\n",
    "    \n",
    "    qa_system_prompt = \"\"\"You are Chatbot, an assistant that offers support for answering questions related to ProCogia. \\\n",
    "                        Chatbot should assume that any query from the User will be regarding ProCogia.com, and so this information does not need to be added.\\\n",
    "                        If Chatbot thinks that the Question is completely irrelevant, then state that you are unable to answer that User query in a reasonable and friendly manner. \\\n",
    "                        You are a support bot for ProCogia first and foremost. \\\n",
    "                        Use the following pieces of retrieved context to answer the question. \\\n",
    "                        Do not answer any question, mathematical equation or anything irrelevant to ProCogia.\\\n",
    "                        If you don't know the answer, just say that you don't know as you are a support bot for ProCogia. \\\n",
    "                        Use three sentences maximum and keep the answer concise.\\\n",
    "                        Input: {context}\n",
    "                        Output: \\\n",
    "                        \"\"\"\n",
    "\n",
    "\n",
    "    qa_prompt = ChatPromptTemplate.from_messages(\n",
    "                [\n",
    "                (\"system\", qa_system_prompt),\n",
    "                MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "                (\"human\", \"{question}\"),\n",
    "                ]\n",
    "                )\n",
    "\n",
    "\n",
    "    def contextualized_question(input: dict):\n",
    "        if input.get(\"chat_history\"):\n",
    "            return contextualize_q_chain\n",
    "        else:\n",
    "            return input[\"question\"]\n",
    "\n",
    "\n",
    "    rag_chain = (\n",
    "            RunnablePassthrough.assign(\n",
    "            context=contextualized_question | retriever | format_docs\n",
    "            )\n",
    "            | qa_prompt\n",
    "            | llm\n",
    "            )\n",
    "\n",
    "    contextualize_q_chain = qa_prompt | llm | StrOutputParser()\n",
    "\n",
    "    ai_msg = rag_chain.invoke({\"question\": question, \"chat_history\": chat_history})\n",
    "    # chat_history.extend([HumanMessage(content=question), ai_msg])\n",
    "    return ai_msg.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182c6b4a-b104-44a3-81a2-a83b31ae3e2a",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "chat_history = []\n",
    "\n",
    "question = \"What is Task Decomposition?\"\n",
    "ai_msg = rag_chain.invoke({\"question\": question, \"chat_history\": chat_history})\n",
    "# chat_history.extend([HumanMessage(content=question), ai_msg])\n",
    "\n",
    "# second_question = \"What are common ways of doing it?\"\n",
    "# rag_chain.invoke({\"question\": second_question, \"chat_history\": chat_history})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc098530-5b22-400f-a372-4cc08d0dadb5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ai_msg.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb1d71a-39db-4dbb-85dc-f5740f647bbc",
   "metadata": {},
   "source": [
    "## Front End Gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3924d6-de8f-491e-b9b4-d817ff7aaeed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9566fa7f-f774-4876-a8fa-7271254f2294",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def q_a_func(query):\n",
    "    template = \"\"\"You are Chatbot, an assistant that offers support for answering questions related to ProCogia. \n",
    "    Chatbot should assume that any query from the User will be regarding ProCogia.com, and so this information does not need to be added.\n",
    "    If Chatbot thinks that the Question is completely irrelevant, then state that you are unable to answer that User query in a reasonable and friendly manner. \n",
    "    You are a support bot for ProCogia first and foremost. \n",
    "    Input: {question}\n",
    "    Output: \n",
    "    \"\"\"\n",
    "    \n",
    "    llm = ChatOpenAI(temperature=0.7, model_name=\"gpt-4\")\n",
    "    memory = ConversationBufferMemory(\n",
    "    memory_key='chat_history', return_messages=True)\n",
    "    conversation_chain = ConversationalRetrievalChain.from_llm(\n",
    "                            llm=llm,\n",
    "                            retriever=docsearch.as_retriever(),\n",
    "                            verbose = True,\n",
    "                            memory=memory\n",
    "                            )\n",
    "    result = conversation_chain({\"question\": query})\n",
    "    x = result[\"answer\"]\n",
    "    print(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2fa85f-2f86-43af-ad8c-a2db426ec989",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import random\n",
    "import time\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    chatbot = gr.Chatbot(label=\"ProCogia Chatbot\",height=800)\n",
    "    msg = gr.Textbox(label=\"Question\")\n",
    "    clear = gr.ClearButton([msg, chatbot])\n",
    "\n",
    "    def respond(message, chat_history):\n",
    "        bot_message = q_a_func(message)\n",
    "        chat_history.append((message, bot_message))\n",
    "        time.sleep(2)\n",
    "        return \"\", chat_history\n",
    "\n",
    "    msg.submit(respond, [msg, chatbot], [msg, chatbot])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c761f35-68e1-4675-b0b6-8ec5a63dbddd",
   "metadata": {},
   "source": [
    "## Front End With Guardrail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b2a151-86b6-43c6-800f-1d3a4463983b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import random\n",
    "import time\n",
    "with gr.Blocks() as demo:\n",
    "    chatbot = gr.Chatbot(label=\"ProCogia Chatbot\",height=800)\n",
    "    msg = gr.Textbox(label=\"Question\")\n",
    "    clear = gr.ClearButton([msg, chatbot])\n",
    "\n",
    "    def respond(message, chat_history):\n",
    "        bot_message = qa_chain_latest(message)\n",
    "        chat_history.append((message, bot_message))\n",
    "        time.sleep(2)\n",
    "        return \"\", chat_history\n",
    "\n",
    "    msg.submit(respond, [msg, chatbot], [msg, chatbot])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ddb4508-180e-499e-91c4-dc9c703b59c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "demo.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad490e8-d51c-48f3-aed6-7a3f70279b74",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# inputs = gr.components.Textbox(label=\"Enter question:\")\n",
    "# answer_box = gr.components.Textbox(label=\"Answer\")\n",
    "\n",
    "# # Create the Gradio interface\n",
    "# demo = gr.Interface(\n",
    "#     fn=q_a_func,\n",
    "#     inputs=\"text\",\n",
    "#     outputs=[answer_box],\n",
    "#     title=\"ProCogia Chatbot\",\n",
    "#     description=\"Enter question\",\n",
    "# ).launch(share=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc59f13-84c2-42fb-ad65-6c6d1130910a",
   "metadata": {},
   "source": [
    "## Example runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443e3f14-ae5e-46e3-a8d5-9a5ea9d3e662",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = \"Who is Giselle Bagatini?\"\n",
    "result = conversation_chain({\"question\": query})\n",
    "answer = result[\"answer\"]\n",
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867d3492-c1ab-4fe4-b5d8-0eda52531897",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = \"What does ProCogia do?\"\n",
    "result = conversation_chain({\"question\": query})\n",
    "answer = result[\"answer\"]\n",
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045b1c54-2f30-4168-846d-de219142cd4a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = \"Who is the Founder of ProCogia?\"\n",
    "result = conversation_chain({\"question\": query})\n",
    "answer = result[\"answer\"]\n",
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7da0024-1515-455b-a2ce-706575880e4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = \"What are the different solutions provided by ProCogia?\"\n",
    "result = conversation_chain({\"question\": query})\n",
    "answer = result[\"answer\"]\n",
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a308586-97b4-4fca-a428-23cd147c9f0c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "llm-poc (smkernel/latest)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:ca-central-1:985470635667:image/smkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
